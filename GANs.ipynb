{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranshtyagi/generative-adversial-networks/blob/main/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP0oR4KYNeks"
      },
      "outputs": [],
      "source": [
        "import numpy  as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdeFsi-nOMMN",
        "outputId": "fd35c491-ab31-4084-fcb5-2812ceaad129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-03 13:00:15--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.136.48, 3.5.134.113, 3.5.135.170, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.136.48|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [binary/octet-stream]\n",
            "Saving to: ‘train-images-idx3-ubyte.gz’\n",
            "\n",
            "train-images-idx3-u 100%[===================>]  25.20M   103MB/s    in 0.2s    \n",
            "\n",
            "2024-10-03 13:00:15 (103 MB/s) - ‘train-images-idx3-ubyte.gz’ saved [26421880/26421880]\n",
            "\n",
            "--2024-10-03 13:00:16--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.136.48, 3.5.134.113, 3.5.135.170, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.136.48|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29515 (29K) [binary/octet-stream]\n",
            "Saving to: ‘train-labels-idx1-ubyte.gz’\n",
            "\n",
            "train-labels-idx1-u 100%[===================>]  28.82K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-10-03 13:00:16 (3.14 MB/s) - ‘train-labels-idx1-ubyte.gz’ saved [29515/29515]\n",
            "\n",
            "--2024-10-03 13:00:16--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.136.48, 3.5.134.113, 3.5.135.170, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.136.48|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4422102 (4.2M) [binary/octet-stream]\n",
            "Saving to: ‘t10k-images-idx3-ubyte.gz’\n",
            "\n",
            "t10k-images-idx3-ub 100%[===================>]   4.22M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-10-03 13:00:16 (34.9 MB/s) - ‘t10k-images-idx3-ubyte.gz’ saved [4422102/4422102]\n",
            "\n",
            "--2024-10-03 13:00:16--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.136.48, 3.5.134.113, 3.5.135.170, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.136.48|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5148 (5.0K) [binary/octet-stream]\n",
            "Saving to: ‘t10k-labels-idx1-ubyte.gz’\n",
            "\n",
            "t10k-labels-idx1-ub 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-03 13:00:16 (229 MB/s) - ‘t10k-labels-idx1-ubyte.gz’ saved [5148/5148]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOogN4c5OQsL",
        "outputId": "ba179bec-48e1-409a-aee9-7e2a947535c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjOrvVnfdJLo"
      },
      "outputs": [],
      "source": [
        "# Training params\n",
        "learning_rate = 0.0002\n",
        "batch_size = 128\n",
        "epochs = 100000\n",
        "\n",
        "# Network params\n",
        "image_dim = 784   # 28x28 image size flattened\n",
        "gen_hidd_dim = 256  # Generator hidden layer dimension\n",
        "disc_hidd_dim = 256 # Discriminator hidden layer dimension\n",
        "z_noise_dim = 100   # Input noise dimension for the generator\n",
        "\n",
        "# Xavier initializer function\n",
        "def xavier_init(shape):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    return initializer(shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDPcN-U_dNy-"
      },
      "outputs": [],
      "source": [
        "# Define weights and bias dictionaries using Xavier initialization\n",
        "weights = {\n",
        "    \"disc_H\": tf.Variable(xavier_init([image_dim, disc_hidd_dim]), trainable=True),\n",
        "    \"disc_final\": tf.Variable(xavier_init([disc_hidd_dim, 1]), trainable=True),\n",
        "    \"gen_H\": tf.Variable(xavier_init([z_noise_dim, gen_hidd_dim]), trainable=True),\n",
        "    \"gen_final\": tf.Variable(xavier_init([gen_hidd_dim, image_dim]), trainable=True)\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"disc_H\": tf.Variable(xavier_init([disc_hidd_dim]), trainable=True),\n",
        "    \"disc_final\": tf.Variable(xavier_init([1]), trainable=True),\n",
        "    \"gen_H\": tf.Variable(xavier_init([gen_hidd_dim]), trainable=True),\n",
        "    \"gen_final\": tf.Variable(xavier_init([image_dim]), trainable=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APa24_uvdWG4"
      },
      "outputs": [],
      "source": [
        "# Define Discriminator function\n",
        "def Discriminator(x):\n",
        "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, weights[\"disc_H\"]), bias[\"disc_H\"]))\n",
        "    final_layer = tf.add(tf.matmul(hidden_layer, weights[\"disc_final\"]), bias[\"disc_final\"])\n",
        "    disc_output = tf.nn.sigmoid(final_layer)\n",
        "    return final_layer, disc_output\n",
        "\n",
        "# Define generator network\n",
        "def Generator(x):\n",
        "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, weights[\"gen_H\"]), bias[\"gen_H\"]))\n",
        "    final_layer = tf.add(tf.matmul(hidden_layer, weights[\"gen_final\"]), bias[\"gen_final\"])\n",
        "    gen_output = tf.nn.sigmoid(final_layer)\n",
        "    return gen_output\n",
        "\n",
        "# Input noise for the generator and real images for the discriminator\n",
        "z_input = tf.random.normal([batch_size, z_noise_dim], name=\"input_noise\")  # Generator input (noise)\n",
        "x_input = tf.random.normal([batch_size, image_dim], name=\"real_input\")     # Discriminator input (real images)\n",
        "\n",
        "# Building the Generator Network\n",
        "with tf.name_scope(\"Generator\"):\n",
        "    output_Gen = Generator(z_input)\n",
        "\n",
        "# Building the Discriminator Network\n",
        "with tf.name_scope(\"Discriminator\"):\n",
        "    real_output1_Disc, real_output_Disc = Discriminator(x_input)        # Real data passed to discriminator\n",
        "    fake_output1_Disc, fake_output_Disc = Discriminator(output_Gen)     # Fake data from generator passed to discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM9Djxuydbuw"
      },
      "outputs": [],
      "source": [
        "# First kind of loss\n",
        "\n",
        "with tf.name_scope(\"Discriminator Loss\") as scope:\n",
        "    Discriminator_Loss = -tf.reduce_mean(tf.math.log(real_output_Disc + 0.0001) + tf.math.log(1. - fake_output_Disc + 0.0001))\n",
        "\n",
        "with tf.name_scope(\"Generator Loss\") as scope:\n",
        "    Generator_Loss = -tf.reduce_mean(tf.math.log(fake_output_Disc + 0.0001))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOhcJGMvdjMk"
      },
      "outputs": [],
      "source": [
        "# Second type of loss\n",
        "with tf.name_scope(\"Discriminator_Loss\") as scope:\n",
        "    Disc_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output1_Disc, labels=tf.ones_like(real_output1_Disc)))\n",
        "    Disc_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc, labels=tf.zeros_like(fake_output1_Disc)))\n",
        "    Discriminator_Loss = Disc_real_loss + Disc_fake_loss\n",
        "\n",
        "with tf.name_scope(\"Generator_Loss\") as scope:\n",
        "    Generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc, labels=tf.ones_like(fake_output1_Disc)))\n",
        "\n",
        "# TensorBoard Summary setup\n",
        "summary_writer = tf.summary.create_file_writer(\"logs/\")\n",
        "\n",
        "# Add summaries to the writer inside a tf.summary context\n",
        "with summary_writer.as_default():\n",
        "    tf.summary.scalar(\"Discriminator_Loss/Real\", Disc_real_loss, step=1)\n",
        "    tf.summary.scalar(\"Discriminator_Loss/Fake\", Disc_fake_loss, step=1)\n",
        "    tf.summary.scalar(\"Discriminator_Loss/Total\", Discriminator_Loss, step=1)\n",
        "    tf.summary.scalar(\"Generator_Loss\", Generator_Loss, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "damv3-Dddriz"
      },
      "outputs": [],
      "source": [
        "# Define the variables\n",
        "Generator_var = [weights[\"gen_H\"], weights[\"gen_final\"], bias[\"gen_H\"], bias[\"gen_final\"]]\n",
        "Discriminator_var = [weights[\"disc_H\"], weights[\"disc_final\"], bias[\"disc_H\"], bias[\"disc_final\"]]\n",
        "\n",
        "# Define the optimizer for Discriminator and Generator\n",
        "Discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "Generator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_NQIsiHdyUZ",
        "outputId": "8430da35-b7ca-4713-8a59-9de79cefd040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Generator Loss: 0.30472517013549805, Discriminator Loss: 1.8519678115844727\n",
            "Epoch: 2000, Generator Loss: 4.121630668640137, Discriminator Loss: 0.01910434290766716\n",
            "Epoch: 4000, Generator Loss: 6.169473648071289, Discriminator Loss: 0.0025293815415352583\n",
            "Epoch: 6000, Generator Loss: 7.583927154541016, Discriminator Loss: 0.0006175156449899077\n",
            "Epoch: 8000, Generator Loss: 9.423749923706055, Discriminator Loss: 0.00011797028128057718\n",
            "Epoch: 10000, Generator Loss: 9.966355323791504, Discriminator Loss: 6.261878297664225e-05\n",
            "Epoch: 12000, Generator Loss: 11.090300559997559, Discriminator Loss: 1.8123817426385358e-05\n",
            "Epoch: 14000, Generator Loss: 8.839189529418945, Discriminator Loss: 0.0002616965211927891\n",
            "Epoch: 16000, Generator Loss: 10.557866096496582, Discriminator Loss: 2.6256362616550177e-05\n",
            "Epoch: 18000, Generator Loss: 8.586808204650879, Discriminator Loss: 0.0001912212319439277\n",
            "Epoch: 20000, Generator Loss: 10.697322845458984, Discriminator Loss: 2.2826829081168398e-05\n",
            "Epoch: 22000, Generator Loss: 10.104940414428711, Discriminator Loss: 5.8794750657398254e-05\n",
            "Epoch: 24000, Generator Loss: 11.301239013671875, Discriminator Loss: 1.2728241927106865e-05\n",
            "Epoch: 26000, Generator Loss: 11.166633605957031, Discriminator Loss: 1.4321163689601235e-05\n",
            "Epoch: 28000, Generator Loss: 11.544480323791504, Discriminator Loss: 9.840026905294508e-06\n",
            "Epoch: 30000, Generator Loss: 9.938801765441895, Discriminator Loss: 4.926200199406594e-05\n",
            "Epoch: 32000, Generator Loss: 10.476216316223145, Discriminator Loss: 5.325728125171736e-05\n",
            "Epoch: 34000, Generator Loss: 10.984981536865234, Discriminator Loss: 1.7785718227969483e-05\n",
            "Epoch: 36000, Generator Loss: 9.746438026428223, Discriminator Loss: 6.0078484239056706e-05\n",
            "Epoch: 38000, Generator Loss: 10.669733047485352, Discriminator Loss: 2.331717587367166e-05\n",
            "Epoch: 40000, Generator Loss: 11.206876754760742, Discriminator Loss: 1.3620111531054135e-05\n",
            "Epoch: 42000, Generator Loss: 12.08568000793457, Discriminator Loss: 5.842566679348238e-06\n",
            "Epoch: 44000, Generator Loss: 11.593587875366211, Discriminator Loss: 9.225946996593848e-06\n",
            "Epoch: 46000, Generator Loss: 11.370773315429688, Discriminator Loss: 1.1528107279445976e-05\n",
            "Epoch: 48000, Generator Loss: 11.38465404510498, Discriminator Loss: 1.1584782441786956e-05\n",
            "Epoch: 50000, Generator Loss: 11.140289306640625, Discriminator Loss: 1.453339064028114e-05\n",
            "Epoch: 52000, Generator Loss: 11.581303596496582, Discriminator Loss: 9.597754797141533e-06\n",
            "Epoch: 54000, Generator Loss: 11.657407760620117, Discriminator Loss: 8.90395403985167e-06\n",
            "Epoch: 56000, Generator Loss: 11.938787460327148, Discriminator Loss: 6.533284249599092e-06\n",
            "Epoch: 58000, Generator Loss: 11.614297866821289, Discriminator Loss: 9.036050869326573e-06\n",
            "Epoch: 60000, Generator Loss: 10.931723594665527, Discriminator Loss: 1.788267763913609e-05\n",
            "Epoch: 62000, Generator Loss: 12.409736633300781, Discriminator Loss: 4.079448444826994e-06\n",
            "Epoch: 64000, Generator Loss: 10.450469970703125, Discriminator Loss: 2.8961092539248057e-05\n",
            "Epoch: 66000, Generator Loss: 11.67445182800293, Discriminator Loss: 8.864419214660302e-06\n",
            "Epoch: 68000, Generator Loss: 12.157645225524902, Discriminator Loss: 5.248572051641531e-06\n",
            "Epoch: 70000, Generator Loss: 12.606761932373047, Discriminator Loss: 3.349480266479077e-06\n",
            "Epoch: 72000, Generator Loss: 12.109949111938477, Discriminator Loss: 5.504556156665785e-06\n",
            "Epoch: 74000, Generator Loss: 11.40522289276123, Discriminator Loss: 1.1906229701708071e-05\n",
            "Epoch: 76000, Generator Loss: 12.383739471435547, Discriminator Loss: 4.202488980808994e-06\n"
          ]
        }
      ],
      "source": [
        "# Training loop using GradientTape\n",
        "for epoch in range(epochs):\n",
        "    x_batch = np.random.normal(size=[batch_size, image_dim]).astype(np.float32)\n",
        "    z_noise = np.random.uniform(-1., 1., size=[batch_size, z_noise_dim]).astype(np.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
        "        real_output1_Disc, real_output_Disc = Discriminator(x_batch)\n",
        "        fake_output1_Disc, fake_output_Disc = Discriminator(Generator(z_noise))\n",
        "\n",
        "        # Compute losses\n",
        "        Disc_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output1_Disc, labels=tf.ones_like(real_output1_Disc)))\n",
        "        Disc_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc, labels=tf.zeros_like(fake_output1_Disc)))\n",
        "        Discriminator_Loss = Disc_real_loss + Disc_fake_loss\n",
        "        Generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc, labels=tf.ones_like(fake_output1_Disc)))\n",
        "\n",
        "    # Compute gradients\n",
        "    Discriminator_gradients = disc_tape.gradient(Discriminator_Loss, Discriminator_var)\n",
        "    Generator_gradients = gen_tape.gradient(Generator_loss, Generator_var)\n",
        "\n",
        "    # Apply gradients\n",
        "    Discriminator_optimizer.apply_gradients(zip(Discriminator_gradients, Discriminator_var))\n",
        "    Generator_optimizer.apply_gradients(zip(Generator_gradients, Generator_var))\n",
        "\n",
        "    # Print the progress\n",
        "    if epoch % 2000 == 0:\n",
        "        print(\"Epoch: {0}, Generator Loss: {1}, Discriminator Loss: {2}\".format(epoch, Generator_loss.numpy(), Discriminator_Loss.numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR7Egd92PRe1"
      },
      "outputs": [],
      "source": [
        "# Create a TensorFlow session\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())  # Initialize all variables\n",
        "\n",
        "# Testing\n",
        "# Generate images from noise, using the generator network.\n",
        "n = 6\n",
        "canvas = np.empty((28 * n, 28 * n))\n",
        "for i in range(n):\n",
        "  # Noise input\n",
        "  Z_noise = np.random.uniform(-1.,1., size = [batch_size, z_noise_dim])\n",
        "  # Generate image from noise.\n",
        "  g = sess.run(output_Gen, feed_dict = {z_input : z_noise})\n",
        "  # Reverse the colours for better display\n",
        "  g = -1 * (g - 1)\n",
        "  for j in range(n):\n",
        "    # Draw the generated digits\n",
        "    canvas[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = g[j].reshape([28, 28])\n",
        "plt.figure(figsize = (n, n))\n",
        "plt.imshow(canvas, origin = \"upper\", cmap = \"gray\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR/LmBxZjWnGFu/ABmVtrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}